\section{Robustness checks of main result} 

In this section, I explore if the effects on price persist when I break up the data by city and property type. I also test to see if minorities have lower prices because they own listings of worse quality. 

\textbf{Effects persist between large cities}

Edelman and Luca found much larger effects of host race just in New York City than I did in data that included all seven cities. A reasonable hypothesis for this is that New York City is driving all of the variation in price, and when other cities are included, where discrimination might not exist, the effects get smaller. To test this, I broke up the effects of host race on listing price by city and controlled for my preferred specification. The results are in Table 8. In general, no single large city in my data set is driving all of the variation in my data. The effects on price are mostly negative for minority hosts, with a few positive coefficients in cities with fewer observations. As expected, New York City and Los Angeles, the cities with the most host diversity and largest sample size, most closely resemble the coefficients from my main result in Table 5. In smaller cities, more than half of the negative effects are significant to various levels, and none of the positive effects are significant. 

However, there are a few outlier coefficients that are most likely driven by low sample size in smaller cities. The coefficients for black hosts are fairly consistent with the combined data in all cities but New Orleans. In New Orleans, a black host is estimated to earn \$18 less for the same kind of listing as a white host, an effect that is statistically significant at the p $<$ .05 level. The coefficients on Hispanic hosts are mixed - in LA, NYC, and Chicago, the coefficients on Hispanic hosts are approximately the same as the combined analysis, while in Austin, New Orleans, and DC, the coefficients are slightly positive. The outlier coefficient is in Nashville, where Hispanic hosts are estimated to earn \$39 less per day than a comparable white host. However, there are only 21 Hispanic hosts in Nashville, so this result is not very generalizable. In LA and NYC, the coefficients of Asian hosts are consistent with the combined data in sign and magnitude. However, they have large coefficients of -\$18 to \$28 in Chicago and Austin, respectively, both of which are significant. The reasoning is similar to Hispanic hosts. 

Generally speaking, the price difference in New York City and Los Angeles is relatively the same. While there are outlier coefficients in smaller cities, it is unlikely that discrimination against Asian and Hispanic hosts in those cities is actually 8 times higher than New York City or Los Angeles. Rather, those cities often have less than 50 hosts in a particular racial category, so any outliers have the potential to skew the coefficients to a much larger degree.  

\textbf{Effects persist between listing types}

Table 9 presents the effects of host race on price, broken down by various listing characteristics such as price, time on market, and property type. I break up the listings by price in the following way: separately for Los Angeles and New York City, I predict the price of a listing based on its property and host characteristics, without host race. I then use this predicted price to break up listings into higher than, and lower than, the mean predicted price in each city. I control for my preferred specification.  

I find that there is much greater price disparity between white and minority hosts among high-priced listings rather than low-priced listings. Column 1 of Table 9 considers price disparities only in listings priced below-average in Los Angeles, and Column 2 considers only above-average priced listings in Los Angeles. The price disparities are much larger for the expensive listings - \$10 for black hosts, \$15 for Hispanic hosts, and \$18 for Asian hosts. By contrast, the coefficients are much smaller, only \$2-3, for the cheaper listings. This pattern in price disparities indicates that discrimination is more pronounced against minority hosts who own more expensive properties than against minority hosts who own cheaper properties. In New York City (Columns 3 and 4), the coefficients for expensive listings are larger than coefficients for cheap listings by about \$6, so this effect is not limited to one city or driven exclusively by city-specific characteristics. One hypothesis for this effect is that all of the discrimination is statistical, in which case the host race isn't as much as a proxy for property value for guests for cheap listings  as it is for expensive listings, which are owned primarily by white hosts. Another explanation is that if a guest is expecting to put up a larger financial investment, they are more selective about which listings they stay at, so any existing discrimination is exacerbated. 

Columns 5 and 6 show that the price disparities are similar for both old and new listings (old listings are defined as those which have been on the market for more than two years). This suggests that the effects of discrimination are \textit{not} erased out by spending a longer time on the market; in other words, minority hosts do not simply ``catch up" to white hosts after a few years. Columns 7-9 break price disparities up by property type. The effects for black hosts across property types get more pronounced the more expensive the property type. While black hosts who own an apartment expect to earn \$5 less per day than white hosts, that number increases to an \$11 loss for black hosts who own houses. This is consistent with the results in Columns 1-4 and with the statistical discrimination hypothesis - if hosts are using race as a proxy for property value, then we should expect guests to discriminate more against minority hosts who own more expensive properties, such as houses.   


\textbf{Prices are not lower because minority hosts have worse reviews} 

Reviews are often critical for the decisions guests make about the listings they book. It is reasonable to expect the quality of a listing's reviews influences the demand, and therefore the price, for that listing. Previous analyses, including Edelman and Luca (2014), involved controlling for the numeric review score of the listing as a proxy for listing ``quality". However, the numeric review score on a listing often carries little information about the real quality of the listing because there is very little variation in the numeric score.\footnote{A low share of guests who review may be a more accurate proxy for low quality, because many users prefer to leave no review rather than a negative review. Review share information, however, is not available, so instead I use Sentimentr to measure quality from reviews.} In my data, 50\% of listings had an average review of $>$ 96 out of 100, and 75\% had an average review score above 91 out of 100.\footnote{This is the case for most online marketplaces. Fradkin, Grewal, and Holtz (2017) study the determinants of review informativeness on Airbnb and find that most reviews, both numeric and text, are positive. In general, reviews tend to reflect real experience of the user.\cite{fradkin}} An Airbnb guest, seeing little variation in the number of stars different hosts have, may instead rely on the text of the reviews to make their booking decision. Since review text allows guests more flexibility in the feedback they give, it may provide a more accurate and nuanced picture of the guest's experience.\footnote{This is because a guest who leaves a text review have the opportunity to use qualifiers like ``but", or ``except", strengthening words like ``really" or ``a lot", etc.} For this reason I use review text instead of the numeric score in the analysis. 

In order to do this, the race, sex, and age of 16,000 reviewers who left reviews for a subset of the Chicago hosts was coded.\footnote{16,000 is 23\% of the total amount of Chicago reviewers in the data set} For each sentence of each review, I used a sentiment-analysis algorithm called Sentimentr to evaluate how positive or negative the sentence is. Sentimentr uses a dictionary of positive and negative words to assign each sentence a sentiment score from -1 to 1, where 1 is a positive sentence, -1 is a negative sentence, and 0 is a neutral sentence carrying no emotion. Unlike other sentiment analysis programs, Sentimentr doesn't merely count the number of good or bad words in a sentence. It also takes into account valence shifters, or words that affect the sentiment-carrying word in the sentence. For example, the algorithm assigns ``I like the listing", ``I \textit{really} like the listing", and ``I like the listing, \textit{but}..." different valence scores because of the presence of valence-shifting words like ``really" and ``but". Sentimentr calculates the correct sentiment 60-70\% of the time as compared to a human grader. One limitation of conducting sentiment analysis in this way, however, is that not every review that a human would consider bad or good carries a sentiment word that the algorithm would pick up. For example, ``The apartment had cockroaches" is certainly a horrible review, but would be given a score of 0 by Sentimentr because it contains no emotion-laden words. 

In Table 12, I regress this sentiment score on the host race, controlling for my preferred specification from Model 4 in Table 5. This means that each coefficient should be read as the standardized review quality, relative to white males, that reviewer of type A gave host of type B.\footnote{Review quality was standardized with mean 0 and standard deviation of 1.} I break up my regressions by the race and sex of the reviewer, varying across the columns of Table 12. The race and sex of the host varies by row. I therefore am able to see any trends in the quality of the review different types of reviewers gave to different types of hosts. 

I find that results were mixed. Overall, white reviewers show little evidence of systematic bias against minority hosts, as measured by the sentiment scores provided by Sentimentr. The reviews that white guests leave for minority hosts do not significantly differ in quality from those they leave for white male hosts. There are stronger effects when considering the quality of the review minority reviewers gave to minority hosts. Black male guests rate Asian hosts almost 4-8 standard deviations above the mean, but rate black women 3 standard deviation lower than the mean. All minority female reviewers, including black females, rate black men worse than they would rate white men who own a similar type of listing. However, all minority male reviewers rate black men anywhere from .5-2 standard deviations higher than they do white men. This suggests that there is some gender-based favoritism between minority reviewers and black male hosts. However, it is important to keep in mind that some of these large, very significant coefficients are suspicious because of small sample sizes - in several thousand Chicago host and reviewer pairs, there are simply not enough black men who stayed with Asian women to be representative of the overall distribution.

In general, there is no one minority group that uniformly has lower quality reviews. Some groups do tend to give other groups far better reviews, but there is no larger pattern of within-gender or within-race bias between hosts and guests that holds for more than one host-guest pair. Overall, there is not enough evidence to substantiate that minority hosts have systematically lower review quality that can explain lower prices. 
